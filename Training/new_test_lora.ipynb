{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ CenQuery: Llama 3 8B LoRA Fine-Tuning\n",
    "**Project:** Indian Census Text-to-SQL  \n",
    "**Hardware:** Runs on T4 GPU (Free Colab Tier)\n",
    "\n",
    "This notebook fine-tunes the `defog/llama-3-sqlcoder-8b` model on your custom Census dataset using QLoRA (4-bit quantization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "We need specific versions of `peft`, `bitsandbytes`, and `transformers` to run 4-bit training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U torch==2.2.1 torchvision torchaudio\n",
    "!pip install -q -U transformers>=4.40.0\n",
    "!pip install -q -U datasets>=2.19.0\n",
    "!pip install -q -U peft>=0.10.0\n",
    "!pip install -q -U bitsandbytes>=0.43.0\n",
    "!pip install -q -U trl>=0.8.6\n",
    "!pip install -q -U accelerate>=0.29.0\n",
    "!pip install -q scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Training Data\n",
    "Upload the **`consolidated_train.jsonl`** file that your team created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Check if file already exists to avoid re-uploading\n",
    "if not os.path.exists('consolidated_train.jsonl'):\n",
    "    print(\"ðŸ“‚ Please upload 'consolidated_train.jsonl'...\")\n",
    "    uploaded = files.upload()\n",
    "    # Rename if necessary to ensure it matches the expected filename\n",
    "    for filename in uploaded.keys():\n",
    "        if filename != 'consolidated_train.jsonl':\n",
    "            os.rename(filename, 'consolidated_train.jsonl')\n",
    "            print(f\"   Renamed {filename} to consolidated_train.jsonl\")\n",
    "else:\n",
    "    print(\"âœ… 'consolidated_train.jsonl' found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run QLoRA Training\n",
    "This script loads the 8B model in 4-bit mode and fine-tunes it on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"defog/llama-3-sqlcoder-8b\"\n",
    "NEW_MODEL_NAME = \"llama-3-8b-census-sql-adapter\"\n",
    "TRAIN_DATA_PATH = \"consolidated_train.jsonl\"\n",
    "OUTPUT_DIR = \"./results\"\n",
    "\n",
    "# LoRA Params\n",
    "LORA_R = 32\n",
    "LORA_ALPHA = 64\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# Training Params\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 2\n",
    "GRAD_ACCUMULATION = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "def train():\n",
    "    print(f\"ðŸš€ Initializing Training for {MODEL_NAME}...\")\n",
    "\n",
    "    # 1. Quantization Config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "\n",
    "    # 2. Load Base Model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        use_cache=False\n",
    "    )\n",
    "    model.config.pretraining_tp = 1\n",
    "\n",
    "    # 3. Load Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # 4. Load Dataset\n",
    "    dataset = load_dataset(\"json\", data_files=TRAIN_DATA_PATH, split=\"train\")\n",
    "    print(f\"âœ… Loaded {len(dataset)} training examples.\")\n",
    "\n",
    "    # 5. LoRA Config\n",
    "    peft_config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    )\n",
    "\n",
    "    # 6. Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_steps=25,\n",
    "        logging_steps=5,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=0.001,\n",
    "        fp16=False,\n",
    "        bf16=False,\n",
    "        max_grad_norm=0.3,\n",
    "        warmup_ratio=0.03,\n",
    "        group_by_length=True,\n",
    "        lr_scheduler_type=\"constant\",\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    # 7. Trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=2048,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "    # 8. Train\n",
    "    print(\"ðŸ”¥ Starting Training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # 9. Save Locally\n",
    "    print(f\"ðŸ’¾ Saving adapter locally to {NEW_MODEL_NAME}...\")\n",
    "    trainer.model.save_pretrained(NEW_MODEL_NAME)\n",
    "    tokenizer.save_pretrained(NEW_MODEL_NAME)\n",
    "    return trainer\n",
    "\n",
    "# Run the function\n",
    "trainer = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Adapter to Google Drive\n",
    "Mount your Google Drive to save the trained model permanently. This way, if Colab disconnects, you don't lose your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import shutil\n",
    "\n",
    "# 1. Mount Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. Define Destination\n",
    "DESTINATION_FOLDER = \"/content/drive/MyDrive/CenQuery_Adapter\"\n",
    "\n",
    "# 3. Copy files\n",
    "print(f\"ðŸ’¾ Copying model files to {DESTINATION_FOLDER}...\")\n",
    "if os.path.exists(DESTINATION_FOLDER):\n",
    "    shutil.rmtree(DESTINATION_FOLDER)\n",
    "shutil.copytree(\"llama-3-8b-census-sql-adapter\", DESTINATION_FOLDER)\n",
    "\n",
    "print(\"âœ… Success! Adapter saved to Google Drive.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}